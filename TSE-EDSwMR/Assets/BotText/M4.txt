Look, at the root node of our tree, The tennisballs are in complete chaos, and you don't know anything to categorize them correctly yet.
In contrast, the leaves in earlier modules were perfectly ordered. To measure this chaos we introduce entropy.
The higher the entropy of a dataset, the more uncertainty is in it.
What do we learn from that? We realize that our goal is, to minimize Entropy as fast as possible, to get an optimized decision tree. 
To measure which filter gives us the best possible improvement on entropy, we introduce Information Gain.  
From now, building the tree, always choose the category, with the highest information gain, and you will end at an optimal decision tree.
We can implement, that behaviour in an algorithm, so the computer always returns the optimal decision tree.
First, you calculate the entropy, and Information Gain, for each category.
Secondly, you choose the category for the separation by the greatest Information Gain. 
Then, you repeat step 1 and 2, until every node has entropy zero. This is called ID3 Algorithm.
It's a very basic algorithm to build a decision tree.